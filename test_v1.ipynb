{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 1: Load Dataset\n",
    "Step 2: Make Dataset Iterable\n",
    "Step 3: Create Model Class\n",
    "Step 4: Instantiate Model Class\n",
    "Step 5: Instantiate Loss Class\n",
    "Step 6: Instantiate Optimizer Class\n",
    "Step 7: Train Model\n",
    "https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/\n",
    "\"\"\"\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as torch_data\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import pandas as pd\n",
    "from data_loader.dataloader import LSTMTSD_Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "\"\"\" DONt CHANGE\n",
    "Parameters for LSTMModel_v0\n",
    "\"\"\"\n",
    "RATIO_SPLIT = 0.9\n",
    "batch_size = 200\n",
    "n_iters = 10000000\n",
    "seq_dim = 1\n",
    "iter = 0\n",
    "input_dim = 10\n",
    "hidden_dim = 100\n",
    "layer_dim = 1\n",
    "output_dim = 3\n",
    "learning_rate = 0.00001\n",
    "horizon = 1\n",
    "window_size= 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset: 1010579\n",
      "val_dataset: 101058\n",
      "test_dataset: 11229\n"
     ]
    }
   ],
   "source": [
    "## Step 1: Load Dataset\n",
    "\n",
    "path = './dataset/'\n",
    "file_list = os.listdir(path)\n",
    "file_list_py = [file for file in file_list if file.endswith('.csv')] ## 파일명 끝이 .csv인 경우\n",
    "\n",
    "## csv 파일들을 DataFrame으로 불러와서 concat\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df = pd.read_csv(\"./dataset/Telegram_1hour_7.csv\")\n",
    "df.insert(2, \"label\", int(0))\n",
    "df_0 = df[[\"Time\", \"Length\", \"label\"]].to_numpy()\n",
    "\n",
    "df = pd.read_csv(\"./dataset/Zoom_1hour_5.csv\")\n",
    "df.insert(2, \"label\", int(1))\n",
    "df_1 = df[[\"Time\", \"Length\", \"label\"]].to_numpy()\n",
    "\n",
    "df = pd.read_csv(\"./dataset/YouTube_1hour_2.csv\")\n",
    "df.insert(2, \"label\", int(2))\n",
    "df_2 = df[[\"Time\", \"Length\", \"label\"]].to_numpy()\n",
    "\n",
    "df_set = np.vstack((df_0, df_1, df_2))\n",
    "df_set = LSTMTSD_Dataset(df_set, window_size=window_size, horizon=horizon, normalize_method=\"z_score\")\n",
    "#dataloader = DataLoader(df_set, batch_size=1, drop_last=False, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(df_set, [int(len(df_set)*RATIO_SPLIT),len(df_set) - int(len(df_set)*RATIO_SPLIT)])\n",
    "val_dataset, test_dataset = torch.utils.data.random_split(val_dataset, [int(len(val_dataset)*RATIO_SPLIT),len(val_dataset) - int(len(val_dataset)*RATIO_SPLIT)])\n",
    "\n",
    "print(\"train_dataset:\", len(train_dataset))\n",
    "print(\"val_dataset:\", len(val_dataset))\n",
    "print(\"test_dataset:\", len(test_dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1979\n"
     ]
    }
   ],
   "source": [
    "## Step 2: Make Dataset Iterable\n",
    "\n",
    "num_epochs = int(n_iters / (len(train_dataset) / batch_size))\n",
    "#num_epochs = int(num_epochs)\n",
    "#num_epochs = 100\n",
    "print(int(num_epochs))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, drop_last=False, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(dataset=val_dataset, batch_size=batch_size, drop_last=False, shuffle=True, num_workers=0)\n",
    "test_loader  = DataLoader(dataset=test_dataset, batch_size=batch_size, drop_last=False, shuffle=True, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "## Step 3: Create Model Class\n",
    "class LSTMModel_v0(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel_v0, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 10])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([3, 100])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "## Step 4: Instantiate Model Class\n",
    "#model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "model = LSTMModel_v0(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "## Step 5: Instantiate Loss Class\n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "## Step 6: Instantiate Optimizer Class\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "len(list(model.parameters()))\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/statsmodels/tsa/stattools.py:677: RuntimeWarning: invalid value encountered in true_divide\n",
      "  acf = avf[: nlags + 1] / avf[0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-20-4336ae07dbe9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum_epochs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m     \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m         \u001B[0minputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mview\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mseq_dim\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_dim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequires_grad_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    433\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sampler_iter\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    434\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 435\u001B[0;31m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    436\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    437\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[0;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    473\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    474\u001B[0m         \u001B[0mindex\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 475\u001B[0;31m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    476\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    477\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 44\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     45\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 44\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     45\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/dataset.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m    270\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    271\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__getitem__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0midx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 272\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mindices\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    273\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    274\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__len__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/LSTM_TSD/data_loader/dataloader.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m     72\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_numpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     73\u001B[0m         \u001B[0;31m#print(x)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 74\u001B[0;31m         \u001B[0mmeta\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_numpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx2meta\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m4\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     75\u001B[0m         \u001B[0;31m#print(\"meta;\", meta)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     76\u001B[0m         \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_numpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtarget_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/LSTM_TSD/data_loader/dataloader.py\u001B[0m in \u001B[0;36mx2meta\u001B[0;34m(x, FFT_NUM, ARIMA_LAGS)\u001B[0m\n\u001B[1;32m    107\u001B[0m     \u001B[0;31m# statiatical : 4\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    108\u001B[0m     prob_vector = [stats.skew(x), stats.kurtosis(x),\n\u001B[0;32m--> 109\u001B[0;31m                    stats.iqr(x), stats.sem(x)]\n\u001B[0m\u001B[1;32m    110\u001B[0m     \u001B[0mprob_vector\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprob_vector\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    111\u001B[0m     \u001B[0mprob_vector\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/scipy/stats/stats.py\u001B[0m in \u001B[0;36msem\u001B[0;34m(a, axis, ddof, nan_policy)\u001B[0m\n\u001B[1;32m   2417\u001B[0m     \u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_chk_asarray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2418\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2419\u001B[0;31m     \u001B[0mcontains_nan\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnan_policy\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_contains_nan\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnan_policy\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2420\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2421\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mcontains_nan\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mnan_policy\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'omit'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/scipy/stats/stats.py\u001B[0m in \u001B[0;36m_contains_nan\u001B[0;34m(a, nan_policy)\u001B[0m\n\u001B[1;32m    222\u001B[0m         \u001B[0;31m# e.g. np.isnan(a).any()\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    223\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0merrstate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minvalid\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'ignore'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 224\u001B[0;31m             \u001B[0mcontains_nan\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misnan\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    225\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    226\u001B[0m         \u001B[0;31m# This can happen when attempting to sum things which are not\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/numpy/core/_ufunc_config.py\u001B[0m in \u001B[0;36m__exit__\u001B[0;34m(self, *exc_info)\u001B[0m\n\u001B[1;32m    437\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    438\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__exit__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mexc_info\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 439\u001B[0;31m         \u001B[0mseterr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moldstate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    440\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcall\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0m_Unspecified\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    441\u001B[0m             \u001B[0mseterrcall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moldcall\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/numpy/core/_ufunc_config.py\u001B[0m in \u001B[0;36mseterr\u001B[0;34m(all, divide, over, under, invalid)\u001B[0m\n\u001B[1;32m    109\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    110\u001B[0m     \u001B[0mpyvals\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mumath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgeterrobj\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 111\u001B[0;31m     \u001B[0mold\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgeterr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    112\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    113\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mdivide\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/numpy/core/_ufunc_config.py\u001B[0m in \u001B[0;36mgeterr\u001B[0;34m()\u001B[0m\n\u001B[1;32m    175\u001B[0m     \u001B[0mres\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'over'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_errdict_rev\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mval\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    176\u001B[0m     \u001B[0mval\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mmaskvalue\u001B[0m \u001B[0;34m>>\u001B[0m \u001B[0mSHIFT_UNDERFLOW\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m&\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 177\u001B[0;31m     \u001B[0mres\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'under'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_errdict_rev\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mval\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    178\u001B[0m     \u001B[0mval\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mmaskvalue\u001B[0m \u001B[0;34m>>\u001B[0m \u001B[0mSHIFT_INVALID\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m&\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    179\u001B[0m     \u001B[0mres\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'invalid'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_errdict_rev\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mval\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "## Step 7: Train Model\n",
    "# Number of steps to unroll\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels, _) in enumerate(train_loader):\n",
    "        inputs = inputs.view(-1, seq_dim, input_dim).requires_grad_()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs) #output: torch.Size([20, 1])\n",
    "        loss = criterion(outputs, labels.type(torch.LongTensor))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "        if iter % 50 == 0:\n",
    "            # Calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through  validation dataset\n",
    "            for inputs, labels, _ in val_loader:\n",
    "                inputs = inputs.view(-1, seq_dim, input_dim)\n",
    "                outputs = model(inputs) #torch.Size([20, 1])\n",
    "                _, predicted = torch.max(outputs.data, 1) #torch.Size([20])\n",
    "                total += labels.size(0)\n",
    "                predicted = predicted.resize(len(outputs), 1).type(torch.LongTensor) #torch.Size([20, 1])\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## CUDA\n",
    "\n",
    "#  Returns a bool indicating if CUDA is currently available.\n",
    "torch.cuda.is_available()\n",
    "#  True\n",
    "\n",
    "#  Returns the index of a currently selected device.\n",
    "torch.cuda.current_device()\n",
    "#  0\n",
    "\n",
    "#  Returns the number of GPUs available.\n",
    "torch.cuda.device_count()\n",
    "#  1\n",
    "\n",
    "#  Gets the name of a device.\n",
    "torch.cuda.get_device_name(0)\n",
    "#  'GeForce GTX 1060'\n",
    "\n",
    "#  Context-manager that changes the selected device.\n",
    "#  device (torch.device or int) – device index to select.\n",
    "torch.cuda.device(0)\n",
    "# Default CUDA device\n",
    "cuda = torch.device('cuda')\n",
    "\n",
    "iter = 0\n",
    "input_dim = 10\n",
    "hidden_dim = 100\n",
    "layer_dim = 1\n",
    "output_dim = 3\n",
    "learning_rate = 0.00001\n",
    "horizon = 1\n",
    "window_size= 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--cpu', action='store_true',help='run in cpu')\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.cpu:\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    device = torch.device('cuda')\n",
    "\n",
    " x = torch.tensor([1., 2.]).to(device)\n",
    "\n",
    "## Step 1: Load Dataset\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df = pd.read_csv(\"./dataset/Telegram_1hour_7.csv\")\n",
    "df.insert(2, \"label\", int(0))\n",
    "df_0 = df[[\"Time\", \"Length\", \"label\"]].to_numpy()\n",
    "\n",
    "df = pd.read_csv(\"./dataset/Zoom_1hour_5.csv\")\n",
    "df.insert(2, \"label\", int(1))\n",
    "df_1 = df[[\"Time\", \"Length\", \"label\"]].to_numpy()\n",
    "\n",
    "df = pd.read_csv(\"./dataset/YouTube_1hour_2.csv\")\n",
    "df.insert(2, \"label\", int(2))\n",
    "df_2 = df[[\"Time\", \"Length\", \"label\"]].to_numpy()\n",
    "\n",
    "df_set = np.vstack((df_0, df_1, df_2))\n",
    "cuda = torch.device('cuda')\n",
    "df_set = LSTMTSD_Dataset(df_set, window_size=window_size, horizon=horizon, normalize_method=\"z_score\")\n",
    "#dataloader = DataLoader(df_set, batch_size=1, drop_last=False, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(df_set, [int(len(df_set)*RATIO_SPLIT),len(df_set) - int(len(df_set)*RATIO_SPLIT)])\n",
    "val_dataset, test_dataset = torch.utils.data.random_split(val_dataset, [int(len(val_dataset)*RATIO_SPLIT),len(val_dataset) - int(len(val_dataset)*RATIO_SPLIT)])\n",
    "\n",
    "print(\"train_dataset:\", len(train_dataset))\n",
    "print(\"val_dataset:\", len(val_dataset))\n",
    "print(\"test_dataset:\", len(test_dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Step 2: Make Dataset Iterable\n",
    "\n",
    "num_epochs = int(n_iters / (len(train_dataset) / batch_size))\n",
    "#num_epochs = int(num_epochs)\n",
    "#num_epochs = 100\n",
    "print(int(num_epochs))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, drop_last=False, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(dataset=val_dataset, batch_size=batch_size, drop_last=False, shuffle=True, num_workers=0)\n",
    "test_loader  = DataLoader(dataset=test_dataset, batch_size=batch_size, drop_last=False, shuffle=True, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Step 3: Create Model Class\n",
    "class LSTMModel_v0(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel_v0, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Step 4: Instantiate Model Class\n",
    "#model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "model = LSTMModel_v0(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "## Step 5: Instantiate Loss Class\n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "## Step 6: Instantiate Optimizer Class\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "len(list(model.parameters()))\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Step 7: Train Model\n",
    "# Number of steps to unroll\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels, _) in enumerate(train_loader):\n",
    "        inputs = inputs.view(-1, seq_dim, input_dim).requires_grad_()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs) #output: torch.Size([20, 1])\n",
    "        loss = criterion(outputs, labels.type(torch.LongTensor))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "        if iter % 50 == 0:\n",
    "            # Calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through  validation dataset\n",
    "            for inputs, labels, _ in val_loader:\n",
    "                inputs = inputs.view(-1, seq_dim, input_dim)\n",
    "                outputs = model(inputs) #torch.Size([20, 1])\n",
    "                _, predicted = torch.max(outputs.data, 1) #torch.Size([20])\n",
    "                total += labels.size(0)\n",
    "                predicted = predicted.resize(len(outputs), 1).type(torch.LongTensor) #torch.Size([20, 1])\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}